{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference (TD) Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD learning was a breackthroug in solving the problem of reward prediction. It simplifies the complex reasoning about the future.\n",
    "* TD combines the current reward and its prediction of the reward at the next moment in time.\n",
    "* At the next moment, the new prediction is compared against what it is expected to be.\n",
    "* The algorithm computes the differences to tune the old prediction with the new prediction\n",
    "Minimizing those differens along time, that is, matching predictions to reality, the whole prediction mechanism continuously improves.\n",
    "\n",
    "At the same time, late 80s and early 90s, neuroscience was trying to understand the behaviour of dopamine neurons. Dopamine neurones are clustered in the midbrain, but from there transmit to other areas distributing relevant messages. It was clear for these neurons that:\n",
    "* Firing had some relationship to reward\n",
    "* Responses depended also on the sensory input\n",
    "* Responses changed as the animals got experience in a given task\n",
    "\n",
    "Some researchers were in the both sides of Neuroscience and AI and noticed that:\n",
    "* Responses in some dopamine neurons represented reward prediction errors\n",
    "* The brain uses a TD learning algorithm\n",
    "\n",
    "Reward prediction error theory of dopamine has become one of the most successful quantitative theories in neuroscience\n",
    "\n",
    "https://deepmind.com/blog/article/Dopamine-and-temporal-difference-learning-A-fruitful-relationship-between-neuroscience-and-AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![agent_environment_interaction_in_mdp](assets/agent_environment_interaction_in_mdp.png)\n",
    "*The agent environment interaction in a MDP (http://incompleteideas.net/book/RLbook2020.pdf#page=70)*\n",
    "\n",
    "**Run** (Episodic or Continuous)\n",
    "$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \\ldots$$\n",
    "\n",
    "**Return** (Discounted sum of future rewards)\n",
    "$$G_t := \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "**Policy**: Action to take at each state\n",
    "\n",
    "**Value function** (state or state-action): estimates the expected future return for each state or state-action pair for a given policy\n",
    "$$v_\\pi(s) := E_\\pi\\left [G_t | S_t = s\\right]$$\n",
    "\n",
    "**Bellman equations**\n",
    "$$v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'}\\sum_r p(s',r|s,a)\\left( r + \\gamma v_\\pi(s') \\right) $$\n",
    "$$q_\\pi(s, a) = \\sum_{s'} \\sum_r p(s',r|s,a)\\left( r + \\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s', a')\\right) $$\n",
    "\n",
    "**Bellman optimality equations**\n",
    "$$v_*(s) = \\max_a \\sum_{s', r} p(s',r|s,a)\\left( r + \\gamma v_*(s') \\right) $$\n",
    "$$q_*(s, a) = \\sum_{s', r} p(s',r|s,a)\\left( r + \\gamma \\max_{a'} q_*(s', a')\\right) $$\n",
    "\n",
    "**Policy iteration**: Recursive policy evaluation and policy improvement.\n",
    "$$\\pi_0 \\rightarrow v_{\\pi_0} \\rightarrow \\pi_1 \\rightarrow v_{\\pi_1} \\rightarrow \\ldots \\rightarrow \\pi_* \\rightarrow v_* $$\n",
    "\n",
    "**Value iteration**: One update operation instead of separate policy evaluation and policy improvement\n",
    "$$v_{k+1}(s) := \\max_a \\sum_{s', r} p(s',r|s,a)\\left( r + \\gamma v_k(s') \\right) $$\n",
    "\n",
    "**Generalized policy iteration (GPI)**: general idea of interacting between policy evaluation and policy improvement independent of \"how much\" of each one is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo methods\n",
    "\n",
    "* Sample-based method\n",
    "* Valid for unavailable or complex environments\n",
    "* Estimate by averaging over multiple returns\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))$$\n",
    "* Updates are done after the end of an episode, so it is used on episodic tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as py\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "py.init_notebook_mode(connected=True)  # Comment for Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prediction problem, the goal is to estimate\n",
    "\n",
    "$$v_\\pi(s) := E[G_t | S_t = s], $$\n",
    "\n",
    "that is the expected return starting from a given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the sample based approximations Monte Carlo and Temporal Difference:\n",
    "\n",
    "$$\\begin{align}\n",
    "V(S_t) \\leftarrow{ }& V(S_t) + \\alpha (G_t - V(S_t)) \\quad \\text{(Monte Carlo)} \\\\\n",
    "V(S_t) \\leftarrow{ }& V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)) \\quad \\text{(Temporal Difference)}\n",
    "\\end{align}$$\n",
    "\n",
    "Thus, for TD, an approximation is done on\n",
    "\n",
    "$$ G_t \\approx R_{t+1} + \\gamma V(S_{t+1}) $$\n",
    "\n",
    "On TD algorithms, to estimate the value $V(S_t)$ of a state, the estimate of the next state $V(S_{t+1})$ is used. The update rule above is called *TD(0)* or *one-step TD* as it is updated looking only at the following state's estimation.\n",
    "\n",
    "![tabular_td0_for_estimating_v](assets/tabular_td0_for_estimating_v.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_td_episode(v, env, pi, alpha, gamma):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = pi.get_action(s)\n",
    "        s_prime, r, done, _ = env.step(a)\n",
    "        v[s] += alpha * (r + gamma * v[s_prime] - v[s])\n",
    "        s = s_prime\n",
    "    v[s] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply TD(0) to predict the values of the states on the FrozenLake environment of size 8x8. The aim of the environment is to move over a grid world until goal is reached (win) or falled into a hole (lose). Moreover, as the world is frozen, trying to go to a direction does not imply it is fulfilled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions are 0:Left, 1:Down, 2:Right, 3:Up. Playing for some steps, can be checked the environment and its randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.step(2))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict values of states for a given policy, it is required a policy to evaluate. In this case, the selected policy that either goes to South or East, both with equal probability 0.5. Other actions are not taken, i.e., have probability 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SouthEastPolicy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, s):\n",
    "        return np.random.choice([1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the previously defined TD(0) algorithm on the Frozen Lake environment to estimate the values of states given the policy that randomly chooses Down or Right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frozen_lake_state_values(seed=2):\n",
    "    np.random.seed(seed)\n",
    "    n_episodes = 10000\n",
    "    env = gym.make(\"FrozenLake8x8-v0\")\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    v = np.zeros(8 * 8)\n",
    "    pi = SouthEastPolicy()\n",
    "    for _ in range(n_episodes):\n",
    "        tabular_td_episode(v, env, pi, alpha=0.3, gamma=1)\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "                    z=v.reshape(8, 8)))\n",
    "    fig.update_layout(\n",
    "        yaxis_autorange=\"reversed\",\n",
    "        title=\"State values on 8x8 Frozen Lake for Down-Right random policy\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frozen_lake_state_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how states that are down and left have the lowest values. Remember that this values are for the fixed policy that always chooses either Down or Right. That area is full of Holes, which means the probability of reaching the Goal from those areas is not probable. On the other hand, states that are on the right have the highest values, as the policy goes to the Goal from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of TD  Prediction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD methods *bootstrap*, that is, they update estimates based on other estimates.\n",
    "* TD methods do not require a model of the environment as in Dynamic Programming.\n",
    "* TD methods are on-line and incremental, unlike Monte Carlo. Waiting until the end of a possibly long episode to update the values be critical. Moreover, continuous tasks that are not naturally splited in episodes.\n",
    "* TD(0) converges to $v_\\pi$ on the mean for a constant step-size if it is sufficiently small, and with probability 1 if the step-size parameter decreases according to the usual stochastic approximation conditions $\\sum_{n=1}^\\infty \\alpha_n(a) = \\infty$ and $\\sum_{n=1}^\\infty \\alpha_n^2(a) < \\infty$ (f.e. $\\alpha_n(a)=1/n$).\n",
    "* In practice, the convergence is usually faster than Monte Carlo methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalk(gym.Env):\n",
    "    def __init__(self, states_to_terminal=3):\n",
    "        super(RandomWalk, self).__init__()\n",
    "        self.nS = 2 * states_to_terminal + 1\n",
    "        self.initial_s = states_to_terminal\n",
    "        self.s = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = self.initial_s\n",
    "        return self.s\n",
    "        \n",
    "    def step(self, a=None):\n",
    "        if self.s in [0, self.nS - 1]:\n",
    "            return self.s, 0, True, {}\n",
    "        self.s += np.random.choice([-1, 1], p=[0.5, 0.5])\n",
    "        if self.s == 0:\n",
    "            reward, done = 0, True\n",
    "        elif self.s == self.nS - 1:\n",
    "            reward, done = 1, True\n",
    "        else:\n",
    "            reward, done = 0, False\n",
    "        return self.s, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        state = np.full(self.nS, 'O')\n",
    "        state[self.s] = 'X'\n",
    "        print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RandomWalk()\n",
    "env.reset()\n",
    "done = False\n",
    "env.render()\n",
    "while done is False:\n",
    "    _, r, done, _ = env.step()\n",
    "    print(f'Reward: {r}')\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performance of TD(0) against Monte Carlo, let's define a function for MC update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_rw_episode(v, env, pi, alpha):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    s_hist = [s]\n",
    "    while not done:\n",
    "        a = pi.get_action(s)\n",
    "        s, r, done, _ = env.step(a)\n",
    "        s_hist.append(s)\n",
    "    v[s] = 0\n",
    "    for s in s_hist[:-1]:\n",
    "        v[s] += alpha * (r - v[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run a experiment on Random Walk environment to:\n",
    "* See the evolution of state values with TD(0)\n",
    "* Compare TD(0) and MC for different $\\alpha$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_walk_td_vs_mc(seed=2):\n",
    "    np.random.seed(seed=seed)\n",
    "    n_episodes = 200\n",
    "    env = RandomWalk()\n",
    "    v_true = [0, 1/6, 2/6, 3/6, 4/6, 5/6, 0]\n",
    "    v = [0, .5, .5, .5, .5, .5, 0]\n",
    "    pi = SouthEastPolicy()  # Policy has no effect\n",
    "    v_record = np.zeros((n_episodes + 1, 7))\n",
    "    v_record[0, :] = v\n",
    "    for i in range(1, n_episodes + 1):\n",
    "        tabular_td_episode(v, env, pi, alpha=0.1, gamma=1)\n",
    "        v_record[i, :] = v\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "            y=v_true[1:6],\n",
    "            x=np.arange(1, 6),\n",
    "            mode='lines',\n",
    "            name=f'True'))\n",
    "    episodes = [0, 1, 10, 100]\n",
    "    for ep in episodes:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            y=v_record[ep, 1:6],\n",
    "            x=np.arange(1, 6),\n",
    "            mode='lines',\n",
    "            name=f'{ep}'))\n",
    "    fig.update_layout(\n",
    "        title='Estimated values after a number of episodes on a single run of TD(0)',\n",
    "        yaxis_title='Estimated value',\n",
    "        xaxis_title='State',\n",
    "        yaxis_range=None,\n",
    "        xaxis_tickmode='linear')\n",
    "    fig.show()\n",
    "    \n",
    "    fig_rmse = go.Figure()\n",
    "    n_runs = 100\n",
    "    n_episodes = 100\n",
    "    v_true = [0, 1/6, 2/6, 3/6, 4/6, 5/6, 0]\n",
    "    # TD runs\n",
    "    for alpha in [0.05, 0.1, 0.15]:\n",
    "        v_record = np.zeros((n_runs, n_episodes + 1, 7))\n",
    "        for run in range(n_runs):\n",
    "            v = [0, .5, .5, .5, .5, .5, 0]\n",
    "            pi = SouthEastPolicy()  # Policy has no effect\n",
    "            v_record[run, 0, :] = v\n",
    "            for ep in range(1, n_episodes + 1):\n",
    "                tabular_td_episode(v, env, pi, alpha=alpha, gamma=1)\n",
    "                v_record[run, ep, :] = v\n",
    "        rmse = np.sqrt(np.sum(np.power(v_record - v_true, 2), axis=2) / 5)\n",
    "        rmse = np.mean(rmse, axis=0)\n",
    "        fig_rmse.add_trace(go.Scatter(\n",
    "            y=rmse,\n",
    "            x=np.arange(n_episodes + 1),\n",
    "            mode='lines',\n",
    "            name=f'TD: {alpha}'))\n",
    "    # MC runs\n",
    "    for alpha in [0.01, 0.02, 0.03, 0.04]:\n",
    "        v_record = np.zeros((n_runs, n_episodes + 1, 7))\n",
    "        for run in range(n_runs):\n",
    "            v = [0, .5, .5, .5, .5, .5, 0]\n",
    "            pi = SouthEastPolicy()  # Policy has no effect\n",
    "            v_record[run, 0, :] = v\n",
    "            for ep in range(1, n_episodes + 1):\n",
    "                mc_rw_episode(v, env, pi, alpha=alpha)\n",
    "                v_record[run, ep, :] = v\n",
    "        rmse = np.sqrt(np.sum(np.power(v_record - v_true, 2), axis=2) / 5)\n",
    "        rmse = np.mean(rmse, axis=0)\n",
    "        fig_rmse.add_trace(go.Scatter(\n",
    "            y=rmse,\n",
    "            x=np.arange(n_episodes + 1),\n",
    "            mode='lines',\n",
    "            line = dict(dash='dash'),\n",
    "            name=f'MC: {alpha}'))\n",
    "    fig_rmse.update_layout(\n",
    "        title='RMSE of state values averaged over states over multiple runs',\n",
    "        xaxis_title='Episode',\n",
    "        yaxis_title='RMSE of state values')\n",
    "    fig_rmse.show()\n",
    "                \n",
    "plot_random_walk_td_vs_mc(seed=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of TD(0) were better for any choice of the parameter $\\alpha$ against any of the MC methods evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Optimality of TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "ADD BATCH UPDATING INFO AND CODE EXAMPLE\n",
    "\n",
    "Under batch updating:\n",
    "* TD(0) converges deterministically to a single answer no matter the choice of a sufficiently small $\\alpha$.\n",
    "* Constant $\\alpha$ MC also fulfills the condition, with another answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration (GPI) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop:\n",
    "  1. Policy Evaluation ($\\pi \\rightarrow V$)\n",
    "  2. Policy Improvement ($V \\rightarrow \\pi$)\n",
    "\n",
    "Previously in MC:\n",
    "* At the end of each episode PE and then PI\n",
    "* PI without a full PE\n",
    "* Evaluate + Improve each episode\n",
    "\n",
    "Now, improve policy after just one Policy Evaluation step, with TD\n",
    "\n",
    "Instead of looking to transitions between states $V(s)$, consider learning transitions between state-action to state-action $Q(s,a)$. This is SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA, on-policy TD control"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![state_action_reward_run](assets/state_action_reward_run.png)\n",
    "*http://incompleteideas.net/book/RLbook2020.pdf#page=153*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha ( R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) $$\n",
    "\n",
    "Actions are taken according to the considered policy (based on Q-values).\n",
    "\n",
    "Note the similarity to he TD update for the state values\n",
    "\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha \\left( R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right) $$\n",
    "\n",
    "This algorithm is just for policy evaluation with state-action values, but considering GPI it becomes a control algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self, q=None, n_states=None, n_actions=None,\n",
    "                 alpha=0.2, gamma=0.9, eps=0.1, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        if q is not None:\n",
    "            self.q = np.array(q)\n",
    "        elif n_states > 0 and n_actions > 0:\n",
    "            self.q = np.zeros((n_states, n_actions))\n",
    "        else:\n",
    "            assert False\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.n_states = self.q.shape[0]\n",
    "        self.n_actions = self.q.shape[1]\n",
    "        \n",
    "        self.last_s = None\n",
    "        self.last_a = None\n",
    "        self.n_steps = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.q = np.zeros((self.n_states, self.n_actions))\n",
    "    \n",
    "    def init(self, s, a):\n",
    "        self.last_s = s\n",
    "        self.last_a = a\n",
    "        self.n_steps = 0\n",
    "        self.reward = 0\n",
    "\n",
    "    def get_action(self, s, eps=None):\n",
    "        eps = self.eps if eps is None else eps\n",
    "        if np.random.rand() < (1 - eps):\n",
    "            return np.random.choice(\n",
    "                np.arange(self.n_actions)[self.q[s] == np.max(self.q[s])])\n",
    "        else:\n",
    "            return np.random.choice(np.arange(self.n_actions))\n",
    "    \n",
    "    def step(self, r, s):\n",
    "        a = self.get_action(s)\n",
    "        self.q[self.last_s, self.last_a] += self.alpha * (\n",
    "            r + self.gamma * self.q[s, a] - self.q[self.last_s, self.last_a])\n",
    "        self.last_s, self.last_a = s, a\n",
    "        self.n_steps += 1\n",
    "        self.reward += r\n",
    "        return s, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA Control in the Windy Grid World Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 2\n",
    "DOWN = 1\n",
    "RIGHT = 3\n",
    "UP = 0\n",
    "\n",
    "WORLD_HEIGHT = 7\n",
    "WORLD_WIDTH = 10\n",
    "\n",
    "WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
    "\n",
    "START = [3, 0]\n",
    "GOAL = [3, 7]\n",
    "\n",
    "\n",
    "class WindyGridWorld(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(WindyGridWorld, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.Discrete(27)\n",
    "        self.world_height = WORLD_HEIGHT\n",
    "        self.world_width = WORLD_WIDTH\n",
    "        self.world_wind = WIND\n",
    "\n",
    "        self.nA = 4\n",
    "        self.nS = WORLD_HEIGHT * WORLD_WIDTH\n",
    "        \n",
    "        self.s = self._to_s(*START)\n",
    "\n",
    "    def _to_s(self, row, col):\n",
    "        return row * WORLD_WIDTH + col\n",
    "    \n",
    "    def _to_pos(self, s):\n",
    "        return [s // WORLD_WIDTH, s % WORLD_WIDTH]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.s = self._to_s(*START)\n",
    "        return self.s\n",
    "\n",
    "    def step(self, a):\n",
    "        i, j = self._to_pos(self.s)\n",
    "        if a == UP:\n",
    "            s_pos = [max(i - 1 - WIND[j], 0), j]\n",
    "        elif a == DOWN:\n",
    "            s_pos = [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), j]\n",
    "        elif a == LEFT:\n",
    "            s_pos = [max(i - WIND[j], 0), max(j - 1, 0)]\n",
    "        elif a == RIGHT:\n",
    "            s_pos = [max(i - WIND[j], 0), min(j + 1, WORLD_WIDTH - 1)]\n",
    "        else:\n",
    "            assert False\n",
    "        self.s = self._to_s(*s_pos)\n",
    "        reward = -1 if s_pos != GOAL else 0\n",
    "        done = False if s_pos != GOAL else True\n",
    "        return self.s, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        world = pd.DataFrame(np.full((WORLD_HEIGHT, WORLD_WIDTH), ' '))\n",
    "        world.iloc[START[0], START[1]] = 'S'\n",
    "        world.iloc[GOAL[0], GOAL[1]] = 'G'\n",
    "        s_pos = self._to_pos(self.s)\n",
    "        world.iloc[s_pos[0], s_pos[1]] = 'X'\n",
    "        display(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WindyGridWorld()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.step(3))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_windy_grid_world(seed=2):\n",
    "    np.random.seed(2)\n",
    "    env = WindyGridWorld()\n",
    "    agent = SarsaAgent(q=np.zeros((env.nS, env.nA)), gamma=1, alpha=0.5)\n",
    "    n_time_steps = 8001 \n",
    "    time_steps = np.arange(n_time_steps)\n",
    "    episodes = np.zeros(n_time_steps)\n",
    "    time_step = 0\n",
    "    while time_step < n_time_steps:\n",
    "        s = env.reset()\n",
    "        a = agent.get_action(s)\n",
    "        agent.init(s, a)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            s, r, done, _ = env.step(a)\n",
    "            _, a = agent.step(r, s)\n",
    "        time_step += agent.n_steps\n",
    "        episodes[time_step:n_time_steps] += 1\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Time Step',\n",
    "        yaxis_title='Episodes solved')\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=episodes,\n",
    "        mode='lines',\n",
    "        name=f'Episodes'))\n",
    "    fig.show()\n",
    "    \n",
    "    ep = pd.DataFrame(np.full((env.world_height, env.world_width), ' '))\n",
    "    ep = ep.append(pd.DataFrame([env.world_wind], index=['Wind']))\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "    a = agent.get_action(s)\n",
    "    agent.init(s, a)\n",
    "    played_steps = 0\n",
    "    while done is False and played_steps < 500:\n",
    "        a = np.argmax(agent.q[s])\n",
    "        s_pos = env._to_pos(s)\n",
    "        ep.iloc[s_pos[0], s_pos[1]] = a\n",
    "        s, r, done, _ = env.step(a)\n",
    "        played_steps += 1\n",
    "\n",
    "    s_pos = env._to_pos(s)\n",
    "    ep.iloc[s_pos[0], s_pos[1]] = 'G'\n",
    "    display(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_windy_grid_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Applying a Monte Carlo method here is not easy. It would be an update until the episode is done, and if trapped in a state it could not finish. With Temporal Difference, estimates are updated during the episode, so the situation is avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning: Off-policy TD Control "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most recent applications of Reinforcement Learning are based on Q-learning algorithm, such as the sounded Atari Games play learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q-learning](assets/q_learning.png)\n",
    "*http://incompleteideas.net/book/RLbook2020.pdf#page=153*\n",
    "\n",
    "The clear difference between Sarsa and Q-learning is on the update rule, which is defined as\n",
    "\n",
    "$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left ( R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa and Q-learning relation to Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sarsa**: $ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha ( R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) $\n",
    "\n",
    "$$ q_\\pi(s,a)=\\sum_{s',r} p(s',r|s,a) \\left(r + \\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s',a') \\right) $$\n",
    "\n",
    "**Q-learning**: $ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left ( R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right) $\n",
    "\n",
    "$$ q_*(s,a) = \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma \\max_{a'} q_\\pi(s',a') \\right)$$\n",
    "\n",
    "So Sarsa is based on the Bellman Equations for state-action values, whereas Q-learning is based on the Bellman Optimality Equations for state-action values. Thus:\n",
    "* Sarsa -> Policy Iteration\n",
    "* Q-learning -> Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearnAgent:\n",
    "    def __init__(self, q=None, n_states=None, n_actions=None,\n",
    "                 alpha=0.2, gamma=0.9, eps=0.1, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        if q is not None:\n",
    "            self.q = np.array(q)\n",
    "        elif n_states > 0 and n_actions > 0:\n",
    "            self.q = np.zeros((n_states, n_actions))\n",
    "        else:\n",
    "            assert False\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.n_states = self.q.shape[0]\n",
    "        self.n_actions = self.q.shape[1]\n",
    "        \n",
    "        self.last_s = None\n",
    "        self.last_a = None\n",
    "        self.n_steps = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.q = np.zeros((self.n_states, self.n_actions))\n",
    "    \n",
    "    def init(self, s, a):\n",
    "        self.last_s = s\n",
    "        self.last_a = a\n",
    "        self.n_steps = 0\n",
    "        self.reward = 0\n",
    "\n",
    "    def get_action(self, s, eps=None):\n",
    "        eps = self.eps if eps is None else eps\n",
    "        if np.random.rand() < (1 - eps):\n",
    "            return np.random.choice(\n",
    "                np.arange(self.n_actions)[self.q[s] == np.max(self.q[s])])\n",
    "        else:\n",
    "            return np.random.choice(np.arange(self.n_actions))\n",
    "    \n",
    "    def step(self, r, s):\n",
    "        a = self.get_action(s)\n",
    "        a_max = self.get_action(s, eps=0)\n",
    "        self.q[self.last_s, self.last_a] += self.alpha * (\n",
    "            r + self.gamma * self.q[s, a_max] - self.q[self.last_s, self.last_a])\n",
    "        self.last_s, self.last_a = s, a\n",
    "        self.n_steps += 1\n",
    "        self.reward += r\n",
    "        return s, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Windy Grid World with Q-learning and Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_windy_grid_world_q_learning_vs_sarsa(alpha_sarsa, alpha_q_learn, n_time_steps=8000, seed=2):\n",
    "    np.random.seed(seed)\n",
    "    env = WindyGridWorld()\n",
    "    gamma = 1\n",
    "    eps = 0.1\n",
    "    n_time_steps = n_time_steps + 1\n",
    "    fig = go.Figure()\n",
    "    for i, alg in enumerate(['Sarsa', 'Q-learning']):\n",
    "        episodes = np.zeros(n_time_steps)\n",
    "        time_steps = np.arange(n_time_steps)\n",
    "        if alg == 'Sarsa':\n",
    "            agent = SarsaAgent(q=np.zeros((env.nS, env.nA)), alpha=alpha_sarsa, gamma=gamma, eps=eps)\n",
    "        elif alg == 'Q-learning':\n",
    "            agent = QLearnAgent(q=np.zeros((env.nS, env.nA)), alpha=alpha_q_learn, gamma=gamma, eps=eps)\n",
    "        time_step = 0\n",
    "        while time_step < n_time_steps:\n",
    "            s = env.reset()\n",
    "            a = agent.get_action(s)\n",
    "            agent.init(s, a)\n",
    "            done = False\n",
    "            while done is False:\n",
    "                s, r, done, _ = env.step(a)\n",
    "                _, a = agent.step(r, s)\n",
    "            time_step += agent.n_steps\n",
    "            episodes[time_step:n_time_steps] += 1  \n",
    "        fig.add_trace(go.Scatter(\n",
    "            y=episodes,\n",
    "            mode='lines',\n",
    "            name=f'{alg}'))\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Time Step',\n",
    "        yaxis_title='Episodes solved')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_windy_grid_world_q_learning_vs_sarsa(alpha_sarsa=0.5, alpha_q_learn=0.5, n_time_steps=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, both algorithms improve similarly and solve episodes at the same rate. Then, Q-learning improves considerably faster than Sarsa for the remaining time steps and achieves a better policy.\n",
    "\n",
    "In fact, the slope of the curves determines that Q-learning has achieved the optimal policy (if slope remains constant, the policy is not improving as it solves episodes at a constant number of steps). However, Sarsa has not achieved the optimal policy yet for 8000 time steps.\n",
    "\n",
    "As $\\alpha=0.5$ could be high for Sarsa, let us run another experiment with a lower step and for longer time steps and see if it works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_windy_grid_world_q_learning_vs_sarsa(alpha_sarsa=0.1, alpha_q_learn=0.5, n_time_steps=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we can see that this time Sarsa improves slower with the lower step $\\alpha$ but it reaches the optimal policy. Actually, both Sarsa and Q-learning lines are parallel, which means that the policy the algorithms have reached is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning in Cliff World Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3\n",
    "HEIGHT, WIDTH = 4, 12\n",
    "\n",
    "START = [3, 0]\n",
    "GOAL = [3, 11]\n",
    "CLIFF = [[3, i] for i in range(1, 11)]\n",
    "\n",
    "\n",
    "class CliffWalking(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CliffWalking, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.Discrete(HEIGHT * WIDTH)\n",
    "        self.height = HEIGHT\n",
    "        self.width = WIDTH\n",
    "\n",
    "        self.nA = 4\n",
    "        self.nS = HEIGHT * WIDTH\n",
    "        \n",
    "        self.s = self._to_s(*START)\n",
    "\n",
    "    def _to_s(self, row, col):\n",
    "        return row * WIDTH + col\n",
    "    \n",
    "    def _to_pos(self, s):\n",
    "        return [s // WIDTH, s % WIDTH]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.s = self._to_s(*START)\n",
    "        return self.s\n",
    "\n",
    "    def step(self, a):\n",
    "        i, j = self._to_pos(self.s)\n",
    "        if a == UP:\n",
    "            s_pos = [max(i - 1, 0), j]\n",
    "        elif a == DOWN:\n",
    "            s_pos = [max(min(i + 1, HEIGHT - 1), 0), j]\n",
    "        elif a == LEFT:\n",
    "            s_pos = [max(i, 0), max(j - 1, 0)]\n",
    "        elif a == RIGHT:\n",
    "            s_pos = [max(i, 0), min(j + 1, WIDTH - 1)]\n",
    "        else:\n",
    "            assert False\n",
    "        if s_pos in CLIFF:\n",
    "            reward = -100\n",
    "            s_pos = START\n",
    "        else:\n",
    "            reward = -1\n",
    "        self.s = self._to_s(*s_pos)\n",
    "        done = False if s_pos != GOAL else True\n",
    "        return self.s, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        world = pd.DataFrame(np.full((HEIGHT, WIDTH), ' '))\n",
    "        world.iloc[START[0], START[1]] = 'S'\n",
    "        world.iloc[GOAL[0], GOAL[1]] = 'G'\n",
    "        for c in CLIFF:\n",
    "            world.iloc[c[0], c[1]] = 'C'\n",
    "        s_pos = self._to_pos(self.s)\n",
    "        world.iloc[s_pos[0], s_pos[1]] = 'X'\n",
    "        display(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalking()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.step(0))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cliff_walking():\n",
    "    n_runs = 100\n",
    "    n_episodes = 501\n",
    "    env = CliffWalking()\n",
    "    gamma = 1\n",
    "    alpha = 0.5\n",
    "    episodes = np.arange(n_episodes)\n",
    "\n",
    "    def run_episodes(alg):\n",
    "        sum_rewards = np.zeros(n_episodes)\n",
    "        if alg == 'Sarsa':\n",
    "            agent = SarsaAgent(q=np.zeros((env.nS, env.nA)), gamma=gamma, alpha=alpha, eps=0.1)\n",
    "        elif alg == 'Q-learning':\n",
    "            agent = QLearnAgent(q=np.zeros((env.nS, env.nA)), gamma=gamma, alpha=alpha, eps=0.1)\n",
    "        for ep in episodes:\n",
    "            s = env.reset()\n",
    "            a = agent.get_action(s)\n",
    "            agent.init(s, a)\n",
    "            done = False\n",
    "            while done is False:\n",
    "                s, r, done, _ = env.step(a)\n",
    "                _, a = agent.step(r, s)\n",
    "            sum_rewards[ep] = agent.reward\n",
    "        return sum_rewards, agent\n",
    "\n",
    "    sum_rewards_q_learn, agents_q_learn = zip(*[run_episodes('Q-learning') for _ in range(n_runs)])\n",
    "    sum_rewards_q_learn = np.mean(sum_rewards_q_learn, axis=0)\n",
    "    sum_rewards_sarsa, agents_sarsa = zip(*[run_episodes('Sarsa') for _ in range(n_runs)])\n",
    "    sum_rewards_sarsa = np.mean(sum_rewards_sarsa, axis=0)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    ep = pd.DataFrame(np.full((env.height, env.width), ''))\n",
    "    ep.iloc[-1, 0] = 'S'\n",
    "    ep.iloc[-1, 1:-1] = 'C'\n",
    "    ep.iloc[-1, -1] = 'G'\n",
    "    for i, alg in enumerate(['Sarsa', 'Q-learning']):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            y=sum_rewards_sarsa if alg =='Sarsa' else sum_rewards_q_learn,\n",
    "            x=episodes,\n",
    "            mode='lines',\n",
    "            name=f'{alg}'))\n",
    "        symbol = 's' if alg == 'Sarsa' else 'q'\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        ep.iloc[-1, 0] += symbol\n",
    "        played_steps = 0\n",
    "        while done is False and played_steps < 500:\n",
    "            if alg == 'Sarsa':\n",
    "                a = stats.mode(\n",
    "                    [agent.get_action(s, eps=0) for agent in agents_sarsa])[0]\n",
    "            else:\n",
    "                a = stats.mode(\n",
    "                    [agent.get_action(s, eps=0) for agent in agents_q_learn])[0]\n",
    "            s, r, done, _ = env.step(a)\n",
    "            s_pos = env._to_pos(s)\n",
    "            ep.iloc[s_pos[0], s_pos[1]] += symbol \n",
    "            played_steps += 1\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Episode',\n",
    "        yaxis_title='Episode sum of rewards',\n",
    "        yaxis_range=[-100, np.max([sum_rewards_q_learn, sum_rewards_sarsa])])\n",
    "    fig.show()\n",
    "    display(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cliff_walking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy and off-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On-policy: Learning \"on\" the target policy. The policy is learnt while it is executed.\n",
    "* Off-policy: Two policies are involved, one that is being learnt and a second one which generates the data.\n",
    "    * Target policy: The policy wanted to learn about.\n",
    "    * Behavior policy: The one that chooses the actions to take, and thus, that gathers the data.\n",
    "    \n",
    "Sarsa is on-policy whereas Q-learning is off-policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TD is a sample-based method, as it is MC\n",
    "* TD bootstraps, that is, updates an estimate towards another estimate\n",
    "* TD has advantages over DP and MC:\n",
    "    * Usually converges faster than MC\n",
    "    * A model is not required as in DP\n",
    "    * TD is fully online and incremental\n",
    "\n",
    "* TD control is based on Bellman equations\n",
    "    * Sarsa uses Bellman equations\n",
    "    * Q-learning uses Bellman optimality equations\n",
    "\n",
    "* Sarsa is an on-policy algorithm\n",
    "* Q-learning is an off-policy algorithm\n",
    "\n",
    "* When measuring the performance on-line, Sarsa can do better because on-line methods take account of their own exploration methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
